# -*- coding: utf-8 -*-
"""Single_perceptronnumpy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pY5HIpjIH8TPwo1b83XsU6i0xTanYb3F
"""

import numpy as np

# Input Features
X = np.array([[0.1], [0.8], [0.3], [0.7], [0.5], [0.6]])  # Shape: (6, 1)
Y = np.array([[0], [1], [0], [1], [0],   [1]])              # Shape: (6, 1)

# Initialize weights
w = np.random.randn(1)  # Randomly initialize weight
b = np.random.randn(1)  # Randomly initialize bias

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Binary Cross-Entropy Loss (not needed for per-sample calculation, but useful for logging)
def binary_cross_entropy(Y_actual, Y_predicted):
    return -np.mean(Y_actual * np.log(Y_predicted) + (1 - Y_actual) * np.log(1 - Y_predicted))

# Hyperparameters
learning_rate = .1
epochs = 10

# Training loop
for epoch in range(epochs):
    total_loss = 0  # Track loss for logging purposes

    for i in range(X.shape[0]):  # Loop through each data point
        x_i = X[i]  # Single input feature (shape: (1,))
        y_i = Y[i]  # Corresponding target (shape: (1,))

        # Forward pass
        z = np.dot(x_i, w) + b  # Linear combination (scalar output)
        y_pred = sigmoid(z)     # Apply sigmoid activation

        # Compute loss for logging
        total_loss += binary_cross_entropy(y_i, y_pred)

        # Backward pass (Gradient Descent)
        dz = y_pred - y_i             # Derivative of loss w.r.t z
        dw = x_i * dz                 # Gradient w.r.t weight
        db = dz                       # Gradient w.r.t bias

        # Update weights and bias
        w -= learning_rate * dw
        b -= learning_rate * db
       #print(f"  y_i: {y_i}  y_pred :{y_pred}  w: {w} dw: {dw}" )

    # Print loss for the epoch
    avg_loss = total_loss / X.shape[0]
    print(f"Epoch {epoch + 1}/{epochs}, w:  {w}   b:  {b}   Loss: {avg_loss}")

# Final weights and bias
print("Final weight:", w)
print("Final bias:", b)

a=np.array([[0.1], [0.3]])
a.shape

print(a.shape[1])

#Learning_rate.size
dw.shape
print("shape of w ", w.shape, "\n", "shape of dw ", dw.shape)

import numpy as np

# Input and output features
X = np.array([[0.1], [0.3], [0.5], [0.6], [0.7], [0.8]])  # Input Features
Y = np.array([[0], [0], [0], [1], [1], [1]])  # Output Features

# Initialize weights and biases
w = np.random.randn(1)  # Single weight (array with shape (1,))
b = np.random.randn()   # Bias (scalar)

# Define the sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define the binary cross-entropy loss function
def binary_cross_entropy(Y_actual, Y_predicted):
    return -np.mean(Y_actual * np.log(Y_predicted) + (1 - Y_actual) * np.log(1 - Y_predicted))

# Hyperparameters
Learning_rate = 0.1
epochs = 5

# Training loop
for epoch in range(epochs):
    # Forward pass
    z = np.dot(X, w) + b       # Linear combination (shape: (6, 1))
    y_pred = sigmoid(z)        # Apply sigmoid activation (shape: (6, 1))

    # Compute the loss
    loss = binary_cross_entropy(Y, y_pred)

    # Backward pass (Gradient Descent)
    dz = y_pred - Y                   # Derivative of loss w.r.t z (shape: (6, 1))
    dw = np.dot(X.T, dz) / X.shape[0]  # Gradient w.r.t weights (shape: (1,))
    db = np.sum(dz) / X.shape[0]       # Gradient w.r.t bias (scalar)

    # Debugging shapes: Check the shapes of dw and db
    print(f"Shape of dw: {dw.shape}, Shape of db: {db.shape}")

    # Update weights and bias
    w -= Learning_rate * dw            # Update weights
    b -= Learning_rate * db            # Update bias

    # Print loss at each epoch
    print(f"Epoch {epoch + 1}, Loss: {loss:.4f}")

# Testing the neuron
test_X = np.array([[0.2], [0.4], [0.7]])  # Test input
test_pred = sigmoid(np.dot(test_X, w) + b)  # Predictions
test_class = (test_pred > 0.5).astype(int)  # Threshold for binary classification

# Print test results
print("\nTest Predictions:")
print("Input:", test_X.flatten())
print("Predicted Probabilities:", test_pred.flatten())
print("Predicted Class:", test_class.flatten())