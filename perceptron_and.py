# -*- coding: utf-8 -*-
"""perceptron_AND.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/195Xw_xEwvr8Bb2VU6iZlfI6YDvWL6HMQ
"""

import numpy as np

# Input and Output for AND gate
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs: (x1, x2)
Y = np.array([[0], [0], [0], [1]])             # Output: y

# Initialize weights and bias
w = np.array([0.0, 0.0])  # Two weights, one for each input
b = 0.0                   # Bias

# Step activation function
def step_activation(z):
    return 1 if z >= 0 else 0

# Hyperparameters
learning_rate = 1
epochs = 10

# Training loop
for epoch in range(epochs):
    total_loss = 0  # Track loss for logging

    for i in range(X.shape[0]):  # Loop through each training example
        x_i = X[i]  # Single input pair (e.g., [0, 1])
        y_i = Y[i]  # Corresponding target output (e.g., [0])

        # Forward pass
        z = np.dot(w, x_i) + b  # Linear combination
        y_pred = step_activation(z)  # Step activation function

        # Compute error
        error = y_i - y_pred  # Error: (target - predicted)
        total_loss += abs(error)  # Accumulate absolute error for logging

        # Backward pass (Weight and bias update)
        dw = learning_rate * error * x_i  # Gradient w.r.t weights
        db = learning_rate * error       # Gradient w.r.t bias

        # Update weights and bias
        w += dw
        b += db

    # Print loss for the epoch
    print(f"Epoch {epoch + 1}/{epochs}, Total Error: {total_loss}")

# Final weights and bias
print("Final weights:", w)
print("Final bias:", b)

# Test the neuron
print("\nTesting AND Gate:")
for i in range(X.shape[0]):
    x_i = X[i]
    z = np.dot(w, x_i) + b
    y_pred = step_activation(z)
    print(f"Input: {x_i}, Predicted Output: {y_pred}")