# -*- coding: utf-8 -*-
"""Sigmodal_GATE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Oj7rq-qeIsT8JwgFwuYfZ6zyD0o2yHiY
"""

import numpy as np

# Input features for AND gate
X = np.array([[0, 0],  # Input (0, 0)
              [0, 1],  # Input (0, 1)
              [1, 0],  # Input (1, 0)
              [1, 1]]) # Input (1, 1)

# Output for AND gate (Expected output)
Y = np.array([[0],  # Output for (0, 0)
              [0],  # Output for (0, 1)
              [0],  # Output for (1, 0)
              [1]]) # Output for (1, 1)

# Initialize weights and bias
w = np.random.randn(2, 1)  # Two weights (for two inputs)
b = np.random.randn()      # Bias

# Define the sigmoid activation function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define the binary cross-entropy loss function (for monitoring the loss)
def binary_cross_entropy(Y_actual, Y_predicted):
    return -np.mean(Y_actual * np.log(Y_predicted) + (1 - Y_actual) * np.log(1 - Y_predicted))

# Hyperparameters
learning_rate = 0.1
epochs = 2000  # You may need to adjust this to ensure convergence

# Training loop
for epoch in range(epochs):
    # Forward pass: compute the output
    z = np.dot(X, w) + b        # Linear combination
    y_pred = sigmoid(z)         # Sigmoid activation (probabilities)

    # Compute the loss
    loss = binary_cross_entropy(Y, y_pred)

    # Backward pass (Gradient Descent)
    dz = y_pred - Y              # Derivative of loss w.r.t. z
    dw = np.dot(X.T, dz) / X.shape[0]  # Gradient w.r.t weights
    db = np.sum(dz) / X.shape[0]       # Gradient w.r.t bias

    # Update weights and bias using gradient descent
    w -= learning_rate * dw
    b -= learning_rate * db

    # Print loss every 100 epochs for monitoring
    if epoch % 100 == 0:
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}")

# Final weights and bias after training
print("\nFinal weights and bias:")
print("Weights:", w)
print("Bias:", b)

# Testing the model
test_X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Test input for AND gate
test_pred = sigmoid(np.dot(test_X, w) + b)  # Predictions using learned weights and bias
test_class = (test_pred > 0.5).astype(int)  # Classify based on threshold of 0.5

# Display the test results
print("\nTest Predictions:")
print("Input:", test_X)
print("Predicted Probabilities:", test_pred.flatten())
print("Predicted Classes:", test_class.flatten())

import numpy as np

# Input features for OR gate
X = np.array([[0, 0],  # Input (0, 0)
              [0, 1],  # Input (0, 1)
              [1, 0],  # Input (1, 0)
              [1, 1]]) # Input (1, 1)

# Output for OR gate (Expected output)
Y = np.array([[0],  # Output for (0, 0)
              [1],  # Output for (0, 1)
              [1],  # Output for (1, 0)
              [1]]) # Output for (1, 1)

# Initialize weights and bias
w = np.random.randn(2, 1)  # Two weights (for two inputs)
b = np.random.randn()      # Bias

# Define the sigmoid activation function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define the binary cross-entropy loss function (for monitoring the loss)
def binary_cross_entropy(Y_actual, Y_predicted):
    return -np.mean(Y_actual * np.log(Y_predicted) + (1 - Y_actual) * np.log(1 - Y_predicted))

# Hyperparameters
learning_rate = 0.1
epochs = 1000  # You may need to adjust this to ensure convergence

# Training loop
for epoch in range(epochs):
    # Forward pass: compute the output
    z = np.dot(X, w) + b        # Linear combination
    y_pred = sigmoid(z)         # Sigmoid activation (probabilities)

    # Compute the loss
    loss = binary_cross_entropy(Y, y_pred)

    # Backward pass (Gradient Descent)
    dz = y_pred - Y              # Derivative of loss w.r.t. z
    dw = np.dot(X.T, dz) / X.shape[0]  # Gradient w.r.t weights
    db = np.sum(dz) / X.shape[0]       # Gradient w.r.t bias

    # Update weights and bias using gradient descent
    w -= learning_rate * dw
    b -= learning_rate * db

    # Print loss every 100 epochs for monitoring
    if epoch % 100 == 0:
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}")

# Final weights and bias after training
print("\nFinal weights and bias:")
print("Weights:", w)
print("Bias:", b)

# Testing the model
test_X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Test input for OR gate
test_pred = sigmoid(np.dot(test_X, w) + b)  # Predictions using learned weights and bias
test_class = (test_pred > 0.5).astype(int)  # Classify based on threshold of 0.5

# Display the test results
print("\nTest Predictions:")
print("Input:", test_X)
print("Predicted Probabilities:", test_pred.flatten())
print("Predicted Classes:", test_class.flatten())

import numpy as np

# Input features for NAND gate
X = np.array([[0, 0],  # Input (0, 0)
              [0, 1],  # Input (0, 1)
              [1, 0],  # Input (1, 0)
              [1, 1]]) # Input (1, 1)

# Output for NAND gate (Expected output)
Y = np.array([[1],  # Output for (0, 0)
              [1],  # Output for (0, 1)
              [1],  # Output for (1, 0)
              [0]]) # Output for (1, 1)

# Initialize weights and bias
w = np.random.randn(2, 1)  # Two weights (for two inputs)
b = np.random.randn()      # Bias

# Define the sigmoid activation function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define the binary cross-entropy loss function (for monitoring the loss)
def binary_cross_entropy(Y_actual, Y_predicted):
    return -np.mean(Y_actual * np.log(Y_predicted) + (1 - Y_actual) * np.log(1 - Y_predicted))

# Hyperparameters
learning_rate = 0.1
epochs = 2000  # You may need to adjust this to ensure convergence

# Training loop
for epoch in range(epochs):
    # Forward pass: compute the output
    z = np.dot(X, w) + b        # Linear combination
    y_pred = sigmoid(z)         # Sigmoid activation (probabilities)

    # Compute the loss
    loss = binary_cross_entropy(Y, y_pred)

    # Backward pass (Gradient Descent)
    dz = y_pred - Y              # Derivative of loss w.r.t. z
    dw = np.dot(X.T, dz) / X.shape[0]  # Gradient w.r.t weights
    db = np.sum(dz) / X.shape[0]       # Gradient w.r.t bias

    # Update weights and bias using gradient descent
    w -= learning_rate * dw
    b -= learning_rate * db

    # Print loss every 100 epochs for monitoring
    if epoch % 100 == 0:
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}")

# Final weights and bias after training
print("\nFinal weights and bias:")
print("Weights:", w)
print("Bias:", b)

# Testing the model
test_X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Test input for NAND gate
test_pred = sigmoid(np.dot(test_X, w) + b)  # Predictions using learned weights and bias
test_class = (test_pred > 0.5).astype(int)  # Classify based on threshold of 0.5

# Display the test results
print("\nTest Predictions:")
print("Input:", test_X)
print("Predicted Probabilities:", test_pred.flatten())
print("Predicted Classes:", test_class.flatten())

import numpy as np

# Input features for NOR gate
X = np.array([[0, 0],  # Input (0, 0)
              [0, 1],  # Input (0, 1)
              [1, 0],  # Input (1, 0)
              [1, 1]]) # Input (1, 1)

# Output for NOR gate (Expected output)
Y = np.array([[1],  # Output for (0, 0)
              [0],  # Output for (0, 1)
              [0],  # Output for (1, 0)
              [0]]) # Output for (1, 1)

# Initialize weights and bias
w = np.random.randn(2, 1)  # Two weights (for two inputs)
b = np.random.randn()      # Bias

# Define the sigmoid activation function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define the binary cross-entropy loss function (for monitoring the loss)
def binary_cross_entropy(Y_actual, Y_predicted):
    return -np.mean(Y_actual * np.log(Y_predicted) + (1 - Y_actual) * np.log(1 - Y_predicted))

# Hyperparameters
learning_rate = 0.1
epochs = 1000  # You may need to adjust this to ensure convergence

# Training loop
for epoch in range(epochs):
    # Forward pass: compute the output
    z = np.dot(X, w) + b        # Linear combination
    y_pred = sigmoid(z)         # Sigmoid activation (probabilities)

    # Compute the loss
    loss = binary_cross_entropy(Y, y_pred)

    # Backward pass (Gradient Descent)
    dz = y_pred - Y              # Derivative of loss w.r.t. z
    dw = np.dot(X.T, dz) / X.shape[0]  # Gradient w.r.t weights
    db = np.sum(dz) / X.shape[0]       # Gradient w.r.t bias

    # Update weights and bias using gradient descent
    w -= learning_rate * dw
    b -= learning_rate * db

    # Print loss every 100 epochs for monitoring
    if epoch % 100 == 0:
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}")

# Final weights and bias after training
print("\nFinal weights and bias:")
print("Weights:", w)
print("Bias:", b)

# Testing the model
test_X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Test input for NOR gate
test_pred = sigmoid(np.dot(test_X, w) + b)  # Predictions using learned weights and bias
test_class = (test_pred > 0.5).astype(int)  # Classify based on threshold of 0.5

# Display the test results
print("\nTest Predictions:")
print("Input:", test_X)
print("Predicted Probabilities:", test_pred.flatten())
print("Predicted Classes:", test_class.flatten())